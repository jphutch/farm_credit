{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "This notebook cleans the census data and merges in several other data sources.\n",
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from simpledbf import Dbf5\n",
    "import shapefile as sp\n",
    "import json\n",
    "#pd.set_option('display.height', 1000)\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('display.width', 1000)\n",
    "\n",
    "with open(\"data_dir.txt\") as f:\n",
    "    data_dir = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agricultural Census \n",
    "\n",
    "[ICPSR 35206](https://www.icpsr.umich.edu/web/ICPSR/studies/35206), collected by Haines, Fishback, and Rhode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhtchns2\\.conda\\envs\\farm_credit\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (1,6,7,9,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,27,28,29,30,31,36,39,41,48,49,50,51,52,54,55,56,58,59,60,61,62,63,64,65,66,67,71,72,73,74,75,76,77,78,84,87,88,92,93,94,95,96,97,98,99,100,101,104,105,106,107,113,114,116,117,118,123,127) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trim down to only Continental US\n",
    "\n",
    "data = pd.read_csv(data_dir + 'clean_data/agcensus_20-40.csv')\n",
    "data.sort_values(['year','county'], inplace=True)\n",
    "data = data[(data.level!=3)] \n",
    "cols = data.columns.tolist()\n",
    "cols1 = cols[:13] + cols[14:]\n",
    "cols1.insert(0,cols[13])\n",
    "df = data[cols1]\n",
    "df = df.query('name!=\"ALASKA\" and name!=\"HAWAII\" and name!=\"DIST COLUMBIA\"')\n",
    "df = df.query('FIPS_state!=15 and FIPS_state!=2 and FIPS_state!=11')\n",
    "#df = df[(df.FIPS!=56047)]\n",
    "# Drop the index number from the merge\n",
    "#df= df.drop('Unnamed: 0',axis=1)\n",
    "df = df.query('county!=9999')\n",
    "\n",
    "df[(df.level ==2)]['name'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.year==1920,'farm_value'] = df.loc[df.year==1920,'farm_value_land'] +\\\n",
    "                                     df.loc[df.year==1920,'farm_value_buildings'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "df = df.iloc[:,:6].join(df.iloc[:,6:].apply(pd.to_numeric,errors='coerce',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corn_acres             5.873256e+04\n",
       "corn_bushels           1.506974e+06\n",
       "corn_grain_acres       4.883862e+04\n",
       "corn_grain_bushels     1.189861e+06\n",
       "corn_silage_acres      2.703593e+03\n",
       "corn_silage_tons       2.068598e+04\n",
       "corn_silage_bushels    1.857962e+04\n",
       "corn_other_acres       9.731131e+03\n",
       "total_corn_bushels5    1.289683e+06\n",
       "total_corn_bushels7    1.305214e+06\n",
       "total_corn_acres       1.028259e+05\n",
       "corn_yield_total5      3.217182e+01\n",
       "corn_yield_total7      3.592808e+01\n",
       "corn_yield5            3.206420e+01\n",
       "corn_yield7            3.583190e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tons of corn silage to bushels\n",
    "# Conversion rate for tons per acre to bushels per acre is from:\n",
    "# http://cdp.wisc.edu/jenny/crop/estimating.pdf\n",
    "\n",
    "\n",
    "df['corn_silage_tonsacres_tem']   = df['corn_silage_tons']/df['corn_silage_acres']\n",
    "df['corn_silage_bushacres_tem']   = df['corn_silage_bushels']/df['corn_silage_acres']\n",
    "df['corn_silage_bush_acres5_tem'] = df['corn_silage_tonsacres_tem'].fillna(0)*5 + df['corn_silage_bushacres_tem'].fillna(0)\n",
    "df['corn_silage_bush_acres7_tem'] = df['corn_silage_tonsacres_tem'].fillna(0)*7 + df['corn_silage_bushacres_tem'].fillna(0)\n",
    "df['corn_grain_bush_acres_tem']   = df['corn_grain_bushels']/df['corn_grain_acres']\n",
    "df['corn_bush_acres_tem']         = df['corn_bushels']/df['corn_acres']\n",
    "\n",
    "df['total_corn_bushels5'] = df['corn_bushels'].fillna(0) + df['corn_grain_bushels'].fillna(0) +\\\n",
    "                           df['corn_silage_bushels'].fillna(0) + df['corn_silage_tons'].fillna(0)*5\n",
    "    \n",
    "df['total_corn_bushels7'] = df['corn_bushels'].fillna(0) + df['corn_grain_bushels'].fillna(0) +\\\n",
    "                            df['corn_silage_bushels'].fillna(0) + df['corn_silage_tons'].fillna(0)*7\n",
    "df['total_corn_acres'] =    df['corn_acres'].fillna(0) + df['corn_grain_acres'].fillna(0) + df['corn_other_acres'].fillna(0) + \\\n",
    "                            df['corn_silage_acres'].fillna(0)\n",
    "df['corn_acres_total_tem'] = df['corn_acres'].fillna(0) + df['corn_grain_acres'].fillna(0) + df['corn_other_acres'].fillna(0) + \\\n",
    "                                df['corn_silage_acres'].fillna(0)\n",
    "df['corn_acres_subtotal_tem'] = df['corn_acres'].fillna(0) + df['corn_grain_acres'].fillna(0) + \\\n",
    "                                df['corn_silage_acres'].fillna(0)\n",
    "\n",
    "df['adjustment_tem'] = df['corn_acres_subtotal_tem'].divide(df['corn_acres_total_tem'],fill_value=0)\n",
    "\n",
    "df['corn_yield_total5']   = (df['corn_bush_acres_tem'].fillna(0) + df['corn_silage_bush_acres5_tem'] + df['corn_grain_bush_acres_tem'].fillna(0))\n",
    "df['corn_yield_total7']   = (df['corn_bush_acres_tem'].fillna(0) + df['corn_silage_bush_acres7_tem'] + df['corn_grain_bush_acres_tem'].fillna(0))\n",
    "df['corn_yield5']         = df['adjustment_tem']*df.corn_yield_total5\n",
    "df['corn_yield7']         = df['adjustment_tem']*df.corn_yield_total7\n",
    "\n",
    "df = df.replace(np.inf, 0)\n",
    "\n",
    "tempsc = list(df.filter(regex=\"corn\").filter(regex=\"tem\").columns)\n",
    "df.drop(tempsc, axis=1, inplace=True)\n",
    "df.filter(regex=\"corn\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wheat_acres                 40576.621113\n",
       "wheat_bushels              571613.432406\n",
       "wheat_thresh_acres          39490.387261\n",
       "wheat_thresh_bushels       509967.487261\n",
       "wheat_winter_acres          23603.183700\n",
       "wheat_winter_bushels       329285.219307\n",
       "wheat_spring_acres           8105.097941\n",
       "wheat_spring_bushels        89999.420632\n",
       "wheat_durummac_acres         4910.984479\n",
       "wheat_durummac_bushels      48963.604213\n",
       "wheat_threshed_acres        29651.769601\n",
       "wheat_grain_acres           32409.147530\n",
       "wheat_grain_bushels        454535.727477\n",
       "wheat_springoth_acres       36299.136752\n",
       "wheat_springoth_bushels    366446.098291\n",
       "wheat_emmersp_acres           116.304965\n",
       "wheat_emmersp_bushels        2286.145121\n",
       "total_wheat_acres           41955.097081\n",
       "total_wheat_bushels        484791.205507\n",
       "wheat_yield                    11.713372\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the yield on wheat by totalling up\n",
    "\n",
    "df['total_wheat_acres']     = df['wheat_acres'].fillna(0) + df['wheat_grain_acres'].fillna(0) + df['wheat_emmersp_acres'].fillna(0) + df['wheat_thresh_acres'].fillna(0) +df['wheat_threshed_acres'].fillna(0)\n",
    "df['total_wheat_bushels']   = df['wheat_bushels'].fillna(0) + df['wheat_durummac_bushels'].fillna(0) + df['wheat_emmersp_bushels'].fillna(0) + df['wheat_spring_bushels'].fillna(0) +df['wheat_winter_bushels'].fillna(0) \n",
    "\n",
    "df['wheat_yield']     = df['total_wheat_bushels'].divide(df['total_wheat_acres'],fill_value=0)\n",
    "df = df.replace(np.inf, 0)\n",
    "\n",
    "tempsw = list(df.filter(regex=\"wheat\").filter(regex=\"tem\").columns)\n",
    "df.drop(tempsw, axis=1, inplace=True)\n",
    "\n",
    "# Above code puts an NA when there should be a zero, this code puts a zero if zero acres are reported.\n",
    "\n",
    "miss = df[(np.isnan(df.wheat_yield)==True)].filter(regex=\"wheat\")\n",
    "df[\"boolsum\"] = df.filter(regex=\"wheat\").notnull().sum(axis=1)\n",
    "df.loc[np.isnan(df.wheat_yield)&df.boolsum>0,'wheat_yield'] = 0\n",
    "\n",
    "df.drop([\"boolsum\"], axis=1, inplace=True)\n",
    "df.filter(regex=\"wheat\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the bean yield\n",
    "beanacre = [x for x in list(df.filter(regex='bean').filter(regex='acre')) if 'soy' not in x and 'bool' not in x and 'total' not in x ]\n",
    "\n",
    "df['total_bean_acres'] = 0\n",
    "df['bean_acre_bool'] = 0\n",
    "for var in beanacre:\n",
    "    df['total_bean_acres'] = df['total_bean_acres'] +df[var].fillna(0)\n",
    "    df['bean_acre_bool'] =( ~np.isnan(df[var])).astype(int) + df['bean_acre_bool']\n",
    "df.loc[df.bean_acre_bool==0,'total_bean_acres'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanbush = [x for x in list(df.filter(regex='bean').filter(regex='bushel')) if 'soy' not in x ]\n",
    "df['total_bean_bshls'] = 0\n",
    "df['bean_bush_bool'] = 0\n",
    "for var in beanbush:\n",
    "    df['total_bean_bshls'] = df['total_bean_bshls'] +df[var].fillna(0)\n",
    "    df['bean_bush_bool'] =( ~np.isnan(df[var])).astype(int) + df['bean_bush_bool']\n",
    "df.loc[df.bean_bush_bool==0,'total_bean_bshls'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybush = [x for x in list(df.filter(regex='soybean').filter(regex='bushel')) if 'bool' not in x and 'total' not in x  ]\n",
    "df['total_soy_bshls'] = 0\n",
    "df['soy_bush_bool'] = 0\n",
    "for var in soybush:\n",
    "    df['total_soy_bshls'] = df['total_soy_bshls'] +df[var].fillna(0)\n",
    "    df['soy_bush_bool'] =( ~np.isnan(df[var])).astype(int) + df['soy_bush_bool']\n",
    "df.loc[df.soy_bush_bool==0,'total_soy_bshls'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soyacres = [x for x in list(df.filter(regex='soybean').filter(regex='acres')) if 'bool' not in x and 'total' not in x  ]\n",
    "df['total_soy_acres'] = 0\n",
    "df['soy_acres_bool'] = 0\n",
    "for var in soyacres:\n",
    "    df['total_soy_acres'] = df['total_soy_acres'] +df[var].fillna(0)\n",
    "    df['soy_acres_bool'] =( ~np.isnan(df[var])).astype(int) + df['soy_acres_bool']\n",
    "df.loc[df.soy_acres_bool==0,'total_soy_acres'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soybean_yield'] = df['total_soy_bshls']/df['total_soy_acres'].replace(np.inf,np.nan) \n",
    "\n",
    "df.drop(list(df.filter(regex='bool').columns),axis=1,inplace=True)\n",
    "df.loc[(df.total_soy_acres==0)|(df.total_soy_bshls==0),'soybean_yield'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animal product value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['animal_rev_1920'] = df['dairy_value'] + df['chicken_egg_value'] + df['wool_value']\n",
    "df['animal_revenue'] = df['dairy_value'] + df['chicken_value'] + df['egg_value'] + df['wool_value']\n",
    "df.loc[df.year==1920,'animal_revenue'] = df['animal_rev_1920']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Data (created in QGIS)\n",
    "### - Climate data\n",
    "[Prism Climate Group](https://prism.oregonstate.edu/), publishes average temperature and rainfall.\n",
    "### - Soil measurements\n",
    "[GAEZ Soil Suitability for Corn and Wheat](http://www.fao.org/nr/gaez/en/), published by the FAO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FIPS']=[str(x).split(\".\")[0].zfill(5) for x in df.FIPS]\n",
    "\n",
    "# Read in the spatial dataset and climate data\n",
    "dbf_precip   = Dbf5(data_dir + 'spatial_data/ppt_total1.dbf',codec='utf-8')\n",
    "dbf_tmean    = Dbf5(data_dir + 'spatial_data/tmean_total.dbf',codec='utf-8')\n",
    "\n",
    "df_ppt = dbf_precip.to_dataframe()\n",
    "df_tmn = dbf_tmean.to_dataframe()\n",
    "\n",
    "# Two extinct Georgia counties have to be taken out\n",
    "df_tmn = df_tmn.loc[~((df_tmn.ICPSRNAM=='MILTON')&(df_tmn.STATENAM=='Georgia'))]\n",
    "df_tmn = df_tmn.loc[~((df_tmn.ICPSRNAM=='CAMPBELL')&(df_tmn.STATENAM=='Georgia'))]\n",
    "\n",
    "df_ppt = df_ppt.loc[~((df_ppt.ICPSRNAM=='MILTON')&(df_ppt.STATENAM=='Georgia'))]\n",
    "df_ppt = df_ppt.loc[~((df_ppt.ICPSRNAM=='CAMPBELL')&(df_ppt.STATENAM=='Georgia'))]\n",
    "\n",
    "df_ppt = df_ppt[['_ppt_mean','_ppt_std','_ppt_var','FIPS','year','ICPSRNAM']]\n",
    "df_ppt.columns = ['rain_mean','rain_std','rain_var','FIPS','year','ICPSRNAM']\n",
    "\n",
    "df_tmn = df_tmn[['_tmn_mean','_tmn_std','_tmn_var','FIPS','year']]\n",
    "df_tmn.columns = ['temp_mean','temp_std','temp_var','FIPS','year']\n",
    "\n",
    "df_ppt_tmn = df_ppt.merge(df_tmn)\n",
    "\n",
    "\n",
    "# Read in soils\n",
    "soils = Dbf5(data_dir + 'spatial_data/soils.dbf', codec='utf-8')\\\n",
    "            .to_dataframe().drop(['wheat_mean','wheat_sum','wheat_coun','wcount','wsum','wmean',\\\n",
    "                                  'corncount','cornsum','wheatcount','wheatsum'],axis=1)\n",
    "soils['FIPS'] = soils.countyid\n",
    "\n",
    "# Two extinct Georgia counties have to be taken out\n",
    "soils = soils.loc[~((soils.ICPSRNAM=='MILTON')&(soils.ICPSRST=='44'))]\n",
    "soils = soils.loc[~((soils.ICPSRNAM=='CAMPBELL')&(soils.ICPSRST=='44'))]\n",
    "\n",
    "df_pts = df_ppt_tmn.merge(soils[['FIPS','cornmean','wheatmean','X_CENTROID','Y_CENTROID','ICPSRST','ICPSRCTY']],how='left',on='FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfmerge = pd.merge(df,df_pts,on=['FIPS','year'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some counties misnamed in spatial data.\n",
    "\n",
    "boolvec = dfmerge['name'] ==dfmerge['ICPSRNAM']\n",
    "name_i = list(boolvec[boolvec==False].index)\n",
    "dfmerge[['name','ICPSRNAM','FIPS']].loc[name_i]\n",
    "name_1 = name_i[:3] +name_i[4:]\n",
    "dfmerge = dfmerge.drop(dfmerge.index[name_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns were misnamed and must be merged in to each other.\n",
    "\n",
    "dfmerge['farm_number'] = pd.concat([dfmerge['farm_number'].dropna(), dfmerge['farms_number'].dropna()]).reindex_like(dfmerge)\n",
    "dfmerge['tenant_number'] = pd.concat([dfmerge['tenant_number'].dropna(), dfmerge['tenants_number'].dropna()]).reindex_like(dfmerge)\n",
    "dfmerge['mort_farms_no'] = pd.concat([dfmerge['mort_farms_no'].dropna(), dfmerge['mort_farms_num'].dropna()]).reindex_like(dfmerge)\n",
    "\n",
    "dfmerge = dfmerge.drop(['farms_number','tenants_number','mort_farms_num'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include 1920 counties\n",
    "FIPS_i = list(dfmerge.FIPS.value_counts()[(dfmerge.FIPS.value_counts()<5)].index)\n",
    "dfmerge = dfmerge[-dfmerge.FIPS.isin(FIPS_i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA information\n",
    "\n",
    "Produced from the notebook \"PCA_calculations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAs = pd.read_csv(data_dir+\"clean_data/PCA_info.csv\")\n",
    "\n",
    "dfmerge['FIPS'] = dfmerge.FIPS.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge = dfmerge.merge(PCAs,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA District Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make district dummies\n",
    "districts= {1:['9','23','25','33','34','36','44','50'],\n",
    "            2:['10','24','42','51','54'],\n",
    "            3:['37','45','13','12'],\n",
    "            4:['39','18','21','47'],\n",
    "            5:['1','28','22'],\n",
    "            6:['17','29','5'],\n",
    "            7:['38','27','55','26'],\n",
    "            8:['56','46','31','19'],\n",
    "            9:['8','35','40','20'],\n",
    "            10:['48'],\n",
    "            11:['6','32','49','4'],\n",
    "            12:['53','41','16','30']}\n",
    "\n",
    "dfmerge['FIPS_state'] = dfmerge.FIPS_state.astype(int).astype(str)\n",
    "dfmerge['pca_district'] = np.zeros((len(dfmerge),1))\n",
    "for i,j in districts.items():\n",
    "    dfmerge.loc[dfmerge.FIPS_state.isin(set(j)),'pca_district'] = int(i)\n",
    "    \n",
    "# Categories to use in analysis\n",
    "dfmerge['pca_district_groups'] = 'None'\n",
    "dfmerge.loc[(dfmerge['pca_district']==3)|(dfmerge['pca_district']==5),'pca_district_groups'] = 'South'\n",
    "dfmerge.loc[dfmerge['pca_district']<3,'pca_district_groups'] = 'Northeast'\n",
    "dfmerge.loc[(dfmerge['pca_district']==4)|(dfmerge['pca_district']==6),'pca_district_groups'] = 'Midwest'\n",
    "dfmerge.loc[(dfmerge['pca_district']>6)&(dfmerge['pca_district']<11),'pca_district_groups'] = 'Great Plains'\n",
    "dfmerge.loc[(dfmerge['pca_district']>10),'pca_district_groups'] = 'West'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA State Level data\n",
    "Collected from the Farm Credit Administration annual reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the state level data\n",
    "pcastate = pd.read_csv(data_dir + \"clean_data/pca_state_vars.csv\")\n",
    "pcastate['FIPS_state'] = pcastate.FIPS_state.astype(str)\n",
    "dfmerge = pd.merge(dfmerge, pcastate, how='left', on= ['FIPS_state','year']).sort_values(['year','FIPS'])\n",
    "dfmerge.iloc[:,-10:]= dfmerge.iloc[:,-10:].fillna(0)\n",
    "dfmerge.drop('NAME', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erosion Data\n",
    "From \"The Enduring Impact of the American Dust Bowl: Short- and Long-Run Adjustments to Environmental Catastrophe\" by Hornbeck (2012).\n",
    "\n",
    "https://www.aeaweb.org/articles?id=10.1257/aer.102.4.1477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion = pd.read_csv(data_dir + \"clean_data/hornbeck12_erosion.csv\")\n",
    "\n",
    "erosion['fips']=[str(x).split(\".\")[0].zfill(5) for x in erosion.fips]\n",
    "\n",
    "erosion['fips'] = erosion['fips'].astype(int)\n",
    "\n",
    "dfmerge = dfmerge.merge(erosion.rename(columns={'fips':'FIPS'}),how='left',on='FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with extinct counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge= dfmerge.loc[~((dfmerge.ICPSRNAM=='MILTON')&(dfmerge.ICPSRST=='44'))]\n",
    "dfmerge= dfmerge.loc[~((dfmerge.ICPSRNAM=='CAMPBELL')&(dfmerge.ICPSRST=='44'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Deal Data\n",
    "\n",
    "#### Source: http://www.u.arizona.edu/~fishback/Published_Research_Datasets.html\n",
    "\n",
    "This data is from the paper \"Can the New Deal's Three R's Be Rehabilitated?: A Program-by-Program, County-by-County Analysis\" by Fishback, Kantor, and Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdeal = pd.read_csv(data_dir + 'newdeal/new_deal_spend.csv').rename(columns={'STATE':'ICPSRST','NDCODE':'ICPSRCTY'})\n",
    "newdeal = newdeal[['ICPSRST','ICPSRCTY','FERA','CWA','WPA','PUBASS','PWAF','PWANF1','PWANF2',\\\n",
    "                     'PRA','PBA','AAA','FCA','FSARR2','FSALO','REA','RFC',\\\n",
    "                     'HOLC','INS','USHALC','USHAH','NDEXP','RELIEF','PUBWOR',\\\n",
    "                     'LOAN','MEAN9628','ROOSMMN2','STD9632','PCTVT32','HTEN73B']]\n",
    "newdeal = newdeal.rename(columns={'NDEXP':'total_grants','RELIEF':'total_relief','PUBWOR':'public_works','LOAN':'total_loans'})\n",
    "newdeal['ICPSRST'] = newdeal['ICPSRST'].astype(str)\n",
    "newdeal['ICPSRCTY'] = newdeal['ICPSRCTY'].astype(str)\n",
    "dfmerge = dfmerge.merge(newdeal, how='left',on=['ICPSRST','ICPSRCTY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Corn data\n",
    "\n",
    "Source: NASS (1945). Agricultural Statistics. Technical report, USDA, Washington, DC. Table 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = pd.read_csv(data_dir+\"clean_data/state_names.csv\")\n",
    "hybridcorn = pd.read_csv(data_dir + 'clean_data/hybrid_corn.csv')\n",
    "hybridcorn = hybridcorn.merge(state_names,how='left').drop_duplicates()\n",
    "\n",
    "dfmerge['FIPS_state'] = dfmerge['FIPS_state'].astype(int)\n",
    "\n",
    "dfmerge = dfmerge.merge(hybridcorn[['FIPS_state','year','corn_hybrid_pct']],how='left',on=['FIPS_state','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDIC Bank Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: ICPSR 7 https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/7?q=fdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv(data_dir + 'fdic/fdicdata.csv')\n",
    "\n",
    "with open(data_dir + 'fdic/fdic_var_labels.json') as f:\n",
    "    tmp = f.read()\n",
    "labels = json.loads(tmp)\n",
    "\n",
    "#dictionary for the new variable names\n",
    "new_names = {\"A_\":\"deps_all\",\"B_\":\"deps_sus\",\\\n",
    "             \"C_\":\"no_banks_all\",\"D_\":\"no_banks_sus\",\\\n",
    "             \"E_\":\"deps_natl\",\"F_\":\"deps_natl_sus\",\\\n",
    "             \"G_\":\"deps_st\",\"H_\":\"deps_st_sus\",\\\n",
    "             \"I_\":\"no_banks_natl\",\"J_\":\"no_banks_natl_sus\",\\\n",
    "             \"K_\":\"no_banks_st\",\"L_\":\"no_banks_st_sus\"}\n",
    "\n",
    "rawdata = rawdata.rename(columns=labels)\n",
    "\n",
    "rawdata['index'] = rawdata.index\n",
    "\n",
    "clean_annual = pd.wide_to_long(rawdata,list(new_names.keys()),i=\"index\",j=\"year_\").rename(columns = new_names)\n",
    "clean_annual = clean_annual.reset_index().drop(['index','YEAR','CATALOG_ENTRY_NMBR','CONG_DIST_NMBR','TABLE_NUMBER'],axis=1).rename(columns ={\"year_\":\"year\",\"COUNTY_NAME\":\"name\",'DATA TYPE':'level'})\n",
    "\n",
    "clean_annual = clean_annual.replace(-9,np.nan)\n",
    "clean_annual = clean_annual.replace(np.inf,np.nan)\n",
    "\n",
    "\n",
    "clean_annual['ID'] = [str(x).split(\".\")[0].zfill(4) for x in clean_annual['ID']]\n",
    "\n",
    "clean_annual['ICPSRST'] = [str(x).zfill(2) for x in clean_annual['ICPSRST'].astype(float).astype(int)]\n",
    "\n",
    "clean_annual['icpsr_id'] = clean_annual['ICPSRST'] + clean_annual['ID']  \n",
    "\n",
    "clean_annual.sort_values(['icpsr_id','year'], inplace=True)\n",
    "\n",
    "clean_annual['no_banks_sus_fd'] = clean_annual.groupby(['icpsr_id'])['no_banks_sus'].transform(lambda x: x.diff())\n",
    "clean_annual.sort_index(inplace=True)\n",
    "clean_annual['deps_all_lag1'] = clean_annual.groupby('icpsr_id')['deps_all'].shift()\n",
    "clean_annual['percent_deps_sus'] = clean_annual['deps_sus'] /clean_annual['deps_all_lag1']\n",
    "clean_annual['pct_deps_sus'] = clean_annual['deps_sus']/clean_annual['deps_all']\n",
    "\n",
    "fdic_losses = clean_annual[(clean_annual.year<1933)&(clean_annual.year>1928)]\\\n",
    "                                .groupby('icpsr_id')['pct_deps_sus'].mean().reset_index()\n",
    "\n",
    "\n",
    "dfmerge['ICPSRCTY_id'] = [str(x).zfill(4) for x in dfmerge['ICPSRCTY']]\n",
    "dfmerge['ICPSRST_id'] = [str(x).zfill(2) for x in dfmerge['ICPSRST']]\n",
    "\n",
    "dfmerge['icpsr_id']   = dfmerge['ICPSRST_id'] + dfmerge['ICPSRCTY_id']\n",
    "\n",
    "dfmerge = dfmerge.drop(['ICPSRCTY_id','ICPSRST_id'],axis=1)\n",
    "\n",
    "fdic_losses['icpsr_id'] = fdic_losses['icpsr_id'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmerge = dfmerge.merge(state_names,how='left').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfmerge.to_csv(data_dir + 'clean_data/fcs_final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
